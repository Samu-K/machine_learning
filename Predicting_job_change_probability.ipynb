{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "name": "Predicting job change probability",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Samu-K/machine_learning/blob/main/Predicting_job_change_probability.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "arashnic_hr_analytics_job_change_of_data_scientists_path = kagglehub.dataset_download('arashnic/hr-analytics-job-change-of-data-scientists')\n",
        "arashnic_job_change_dataset_answer_path = kagglehub.dataset_download('arashnic/job-change-dataset-answer')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "IYv6us23oQoD"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports and reading in data"
      ],
      "metadata": {
        "id": "HJxVNLe8oQoE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#For handling data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import regex as re\n",
        "\n",
        "#For vizualization of data\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Our ML algos\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "import lightgbm as lgb\n",
        "\n",
        "#SMOTE for imbalanced data\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "#Imputing\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "#Encoders\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "from category_encoders import MEstimateEncoder\n",
        "\n",
        "#Splitting data into training and test\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#Eval functions and model analysis\n",
        "import eli5\n",
        "import shap\n",
        "from eli5.sklearn import PermutationImportance\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "#Hyperparameter tuning\n",
        "from bayes_opt import BayesianOptimization\n",
        "from skopt  import BayesSearchCV\n",
        "\n",
        "#Get rid of futurewarnings\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "DMraueFQoQoF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fp_train = \"../input/hr-analytics-job-change-of-data-scientists/aug_train.csv\"\n",
        "fp_test = \"../input/hr-analytics-job-change-of-data-scientists/aug_test.csv\"\n",
        "\n",
        "train = pd.read_csv(fp_train)\n",
        "test = pd.read_csv(fp_test)\n",
        "print(train.describe())\n",
        "print(test.describe())"
      ],
      "metadata": {
        "trusted": true,
        "id": "yReWp1DMoQoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploratory data analysis (EDA)"
      ],
      "metadata": {
        "id": "6bXchuXYoQoG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Start by taking a general look at the columns"
      ],
      "metadata": {
        "id": "a-wtkPjjoQoG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_cols = train.columns\n",
        "test_cols = test.columns\n",
        "\n",
        "print(\"Training columns are:\\n {}\".format(train_cols))\n",
        "print(\"Testing columns are: \\n {}\".format(test_cols))"
      ],
      "metadata": {
        "trusted": true,
        "id": "_PSuMRzaoQoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at the data to examine data types and also see what they've been classified as by pandas"
      ],
      "metadata": {
        "id": "n8VdCRxjoQoG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train"
      ],
      "metadata": {
        "trusted": true,
        "id": "cc5RkFxToQoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.dtypes"
      ],
      "metadata": {
        "trusted": true,
        "id": "aNnxU1iJoQoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using this info we'll manually make a list of the categorical columns and numerical columns"
      ],
      "metadata": {
        "id": "AQl5_Ex8oQoH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cat_cols = [\"city\",\"gender\",\"relevent_experience\",\"enrolled_university\",\"education_level\",\"major_discipline\",\"experience\",\"company_size\",\"company_type\",\"last_new_job\"]\n",
        "num_cols = [\"enrollee_id\", \"city_development_index\",\"training_hours\"]"
      ],
      "metadata": {
        "trusted": true,
        "id": "bbifzBOWoQoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's also take a look at our missing values and asses if we should drop them, or impute them."
      ],
      "metadata": {
        "id": "Ktzf4VGGoQoH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nulls = train.isnull().sum().sum()\n",
        "nn = train.notnull().sum().sum()\n",
        "null_rows = train.shape[0] - train.dropna().shape[0]\n",
        "total = nulls+nn\n",
        "\n",
        "print(\"Training data:\")\n",
        "print(\"Total null values: {null}\" \"\\n\" \"Percent of all values: {prc}\".format(null=nulls,prc=nulls/nn))\n",
        "print(\"Total null rows: {nr}\" \"\\n\" \"Percent of all rows: {rprc}\".format(nr=null_rows,rprc=null_rows / train.shape[0]))\n",
        "\n",
        "print(\"\\nTesting data:\")\n",
        "nulls = test.isnull().sum().sum()\n",
        "nn = test.notnull().sum().sum()\n",
        "null_rows = test.shape[0] - test.dropna().shape[0]\n",
        "total = nulls+nn\n",
        "\n",
        "print(\"Total null values: {null}\" \"\\n\" \"Percent of all values: {prc}\".format(null=nulls,prc=nulls/nn))\n",
        "print(\"Total null rows: {nr}\" \"\\n\" \"Percent of all rows: {rprc}\".format(nr=null_rows,rprc=null_rows / test.shape[0]))"
      ],
      "metadata": {
        "trusted": true,
        "id": "9EwbXGGOoQoH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that both of the datasets have a massive amount of rows with missing data. \\\n",
        "This means that just dropping missing data would harm the model quite a lot.\n",
        "\n",
        "What we'll do instead is look at each column and how much nan values affect it and how we can impute them."
      ],
      "metadata": {
        "id": "ZwSUrGgioQoI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# As a reminder take a look at the columns\n",
        "train.columns"
      ],
      "metadata": {
        "trusted": true,
        "id": "Ac5WzY4xoQoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`enrollee_id` should have no nan values so we can skip that column. \\\n",
        "Next we'll see how many nulls each column has in both datasets"
      ],
      "metadata": {
        "id": "jsXUqTHMoQoI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for column in train.columns:\n",
        "    # Only training data has target, skip that\n",
        "    if column == \"target\":\n",
        "        continue\n",
        "\n",
        "    print(\"train.{col} nulls: {training}\" \"\\ntest.{col} nulls {test}\\n\".format(\n",
        "        training=train[column].isnull().sum(),\n",
        "        test=test[column].isnull().sum(),\n",
        "        col = column))"
      ],
      "metadata": {
        "trusted": true,
        "id": "g_QbPIZQoQoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The worst offenders off the list are: \\\n",
        "`gender` \\\n",
        "`major_discipline` \\\n",
        "`company_size` \\\n",
        "`company_type`\n",
        "\n",
        "Since there's only a few different values for this feature and the missing values are a minority we can simply fill the missing values with a placeholder value. \\\n",
        "\n",
        "Next `major_discipline`. By taking a look at the data we can see that most of the values are \"STEM\". The next largest value is missing. Because of this we'll just drop the column \\\n",
        "\n",
        "`company_size` and `company_type` are both linked to each other quite strongly. Indeed we see that they have almost the exact same amount of missing values. What we will do, is first encode `company_size`. Then we will impute the missing values. For `company_type` we'll simply fill \"missing\""
      ],
      "metadata": {
        "id": "ehXKm8fGoQoI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First fill gender and company type missing\n",
        "train[\"gender\"].fillna(\"Missing\",inplace=True)\n",
        "test[\"gender\"].fillna(\"Missing\",inplace=True)\n",
        "train.company_type.fillna(\"Missing\",inplace=True)\n",
        "test.company_type.fillna(\"Missing\",inplace=True)\n",
        "\n",
        "#Drop major_discipline\n",
        "train.drop(\"major_discipline\",axis=1,inplace=True)\n",
        "test.drop(\"major_discipline\",axis=1,inplace=True)"
      ],
      "metadata": {
        "trusted": true,
        "id": "bfeuuFA0oQoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Labeling and OneHotEncoding categorical data"
      ],
      "metadata": {
        "id": "7j8eOmn_oQoI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some of the categorical columns are already very close to being numerical (such as `last_new_job`) \\\n",
        "We'll do some regex to turn these into numerical values. \\\n",
        "For the rest, we'll use OH-encoding."
      ],
      "metadata": {
        "id": "m517QC5uoQoJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LabelEncoding\n",
        "\n",
        "The only column we'll LabelEncode is `company_size`. We can label encode this column because the company size does follow a relationship from a smaller size to a bigger one. LabelEncoding this lets the model learn that relationship better."
      ],
      "metadata": {
        "id": "GAfYzxkuoQoJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Drop null so we don't encode those\n",
        "train_dropped = train.dropna(subset=[\"company_size\"])\n",
        "test_dropped = test.dropna(subset=[\"company_size\"])"
      ],
      "metadata": {
        "trusted": true,
        "id": "rWk902WkoQoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to allow the model to learn the data in a numerical form when label encoded we need to make sure that as the label encoded values go up, it corresponds in the company size going up. That means that we need to setup the classes in the correct order for the encoder.\n",
        "\n",
        "We coould type them out manually, but that in a real world application that would mean any new value for `company_size` would break the code.\n",
        "\n",
        "We're going to automate the process of creating the list, so that it will always be up to date and in order."
      ],
      "metadata": {
        "id": "e7GeYCLaoQoJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Start by creating a dict to store the values and their numerical version in\n",
        "values = {}\n",
        "\n",
        "#Loop through the unique values\n",
        "for value in train_dropped.company_size.unique():\n",
        "\n",
        "    #Check to see if the value has a less than mark\n",
        "    if re.search(\"[<]\",value):\n",
        "        #We replace any non-numerical character with whitespace\n",
        "        num = re.sub(\"\\D\",\" \",value)\n",
        "        #We split the string by white spaces. This splits the string into the given numbers\n",
        "        spl = num.strip().split(\" \")\n",
        "\n",
        "        #We take the first number, a.k.a the start of the range and remove one from it\n",
        "        #We do this so that we can more effectively sort the values\n",
        "        n = int(spl[0])-1\n",
        "        #Set the value into the dict, with the numerical version as key\n",
        "        values[n] = value\n",
        "        continue\n",
        "\n",
        "    #This time we check if value has a greater than mark\n",
        "    if re.search(\"[>]\",value):\n",
        "        #Same process to find and split the number\n",
        "        num = re.sub(\"\\D\",\" \",value)\n",
        "        spl = num.strip().split(\" \")\n",
        "        #This time we add one.\n",
        "        n = int(spl[0])+1\n",
        "        values[n] = value\n",
        "        continue\n",
        "\n",
        "    #If the value is just a purely numerical range\n",
        "    #We don't need to modify it in any way\n",
        "    #Just find it\n",
        "    num = re.sub(\"\\D\",\" \",value)\n",
        "    spl = num.strip().split(\" \")\n",
        "    values[int(spl[0])] = value\n",
        "\n",
        "\n",
        "values"
      ],
      "metadata": {
        "trusted": true,
        "id": "LdlLIG2XoQoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Setup a list of the classes we'll feed the encoder\n",
        "classes = []\n",
        "\n",
        "#We keep looping as long as the dict has values\n",
        "while values:\n",
        "    #Find the smallest key value\n",
        "    min_key = min(values.keys())\n",
        "    #Find the class that matches the smaller key value\n",
        "    cl = values[min_key]\n",
        "    #We set this into the list\n",
        "    classes.append(cl)\n",
        "    #And remove it from the dict\n",
        "    del values[min_key]\n",
        "\n",
        "classes"
      ],
      "metadata": {
        "trusted": true,
        "id": "lzKzoLYloQoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have a list of the classes ordered from the smallest company size to the largest."
      ],
      "metadata": {
        "id": "MfqTMzQfoQoJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Set the encode number\n",
        "count = 0\n",
        "\n",
        "#Setup columns for encoding\n",
        "train_dropped[\"company_size_encoded\"] = train_dropped.company_size\n",
        "test_dropped[\"company_size_encoded\"] = test_dropped.company_size\n",
        "\n",
        "#Go through each class in order\n",
        "for cl in classes:\n",
        "    #Replace the matching class with the encode\n",
        "    train_dropped.company_size_encoded.replace(cl,count,inplace=True)\n",
        "    test_dropped.company_size_encoded.replace(cl,count,inplace=True)\n",
        "    #Up the encode\n",
        "    count += 1"
      ],
      "metadata": {
        "trusted": true,
        "id": "41V4M3O1oQoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#With the encoding done we can drop the normal column\n",
        "train_dropped.drop(\"company_size\",axis=1,inplace=True)\n",
        "test_dropped.drop(\"company_size\",axis=1,inplace=True)"
      ],
      "metadata": {
        "trusted": true,
        "id": "CLV0mPKOoQoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# By joining the encoded column into the original dataframe\n",
        "# We get the encoded values for non-null rows, and null for rows that had originally null\n",
        "train = train.join(train_dropped.company_size_encoded)\n",
        "test = test.join(test_dropped.company_size_encoded)"
      ],
      "metadata": {
        "trusted": true,
        "id": "daPFLMbUoQoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we can impute the encoded row missing values\n",
        "\n",
        "# Setup imputer\n",
        "si = SimpleImputer(strategy=\"median\")\n",
        "\n",
        "#Fit and transform training set\n",
        "train[\"company_size_encoded\"] = si.fit_transform(np.array(train[\"company_size_encoded\"]).reshape(-1,1))\n",
        "#Transform test set\n",
        "test[\"company_size_encoded\"] = si.transform(np.array(test[\"company_size_encoded\"]).reshape(-1,1))\n",
        "\n",
        "# Now we can drop the original company_size column\n",
        "train.drop(\"company_size\",axis=1,inplace=True)\n",
        "test.drop(\"company_size\",axis=1,inplace=True)"
      ],
      "metadata": {
        "trusted": true,
        "id": "VFnSI4ngoQoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OneHotEncoding manually"
      ],
      "metadata": {
        "id": "3N6KjJJzoQoK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're going to manually OneHotEncode experience,relevent_experience and last_new_job \\\n",
        "There's no point in encoding every single value, since the column is a combination of categorical and numerical data \\\n",
        "These have only a few values that need encoding, we'll turn the rest into numerical values"
      ],
      "metadata": {
        "id": "N8qCXi6aoQoK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Experience\n",
        "\n",
        "Start by looking at the unique values within the column."
      ],
      "metadata": {
        "id": "19bG2p3BoQoK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train.experience.unique()"
      ],
      "metadata": {
        "trusted": true,
        "id": "IkqZPYUloQoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We then setup a function to manually OH-encode experience. \\\n",
        "We do this so we can easily apply this to the train and test data"
      ],
      "metadata": {
        "id": "7Fl56_ufoQoL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def experience_oh(df):\n",
        "    \"\"\"\n",
        "    OneHotEncode the experience column\n",
        "    \"\"\"\n",
        "    #Create a new column with the >20 values and same with <1\n",
        "    df[\"experience_>20\"] = df.experience.eq(\">20\")\n",
        "    df[\"experience_<1\"] = df.experience.eq(\"<1\")\n",
        "\n",
        "    #Then replace the values in the original column with nan\n",
        "    df[\"experience\"] = df.experience.apply(lambda x: np.nan if x in [\">20\",\"<1\"] else x)\n",
        "\n",
        "    return df[\"experience\"]"
      ],
      "metadata": {
        "trusted": true,
        "id": "uYOLOXJgoQoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Apply the function to both dataframes\n",
        "train[\"experience\"] = experience_oh(train)\n",
        "test[\"experience\"] = experience_oh(test)\n",
        "#Check to see if got rid of wanted values\n",
        "print(train.experience.unique())\n",
        "print(test.experience.unique())"
      ],
      "metadata": {
        "trusted": true,
        "id": "0a-FNTBdoQoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### last_new_job\n",
        "\n",
        "Similar process to experience. \\\n",
        "Start by checking unique values"
      ],
      "metadata": {
        "id": "477jxnR2oQoL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train.last_new_job.unique()"
      ],
      "metadata": {
        "trusted": true,
        "id": "FXD6pfBwoQoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def last_job_oh(df):\n",
        "    \"\"\"\n",
        "    OneHotEncode the last_new_job column smartly\n",
        "    \"\"\"\n",
        "    df[\"last_new_job_>4\"] = df.last_new_job.eq(\">4\")\n",
        "    df[\"last_new_job_never\"] = df.last_new_job.eq(\"never\")\n",
        "\n",
        "\n",
        "    df[\"last_new_job\"] = df.last_new_job.apply(lambda x: np.nan if x in [\">4\",\"never\"] else x)\n",
        "\n",
        "    return df[\"last_new_job\"]"
      ],
      "metadata": {
        "trusted": true,
        "id": "FsKB6oE4oQoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Apply function to both dataframes\n",
        "train[\"last_new_job\"] = last_job_oh(train)\n",
        "test[\"last_new_job\"] = last_job_oh(test)\n",
        "#Check to see if got rid of wanted values\n",
        "print(train.last_new_job.unique())\n",
        "print(test.last_new_job.unique())"
      ],
      "metadata": {
        "trusted": true,
        "id": "PX9UJKYdoQoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dropping nan values\n",
        "We drop nan values instead of imputing them bc we created nan values in the manual encoding. Imputing these would yield bad results."
      ],
      "metadata": {
        "id": "L8nxTRLYoQoM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Drop nan from both dataframes\n",
        "train.dropna(inplace=True)\n",
        "test.dropna(inplace=True)\n",
        "train.head()"
      ],
      "metadata": {
        "trusted": true,
        "id": "r_Kfk8k9oQoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### relevent_experience\n",
        "\n",
        "Similar to earlier steps, but a little simpler. This column only has two values, which we can describe with just 0 or 1. \\\n",
        "As always start with checking unique values"
      ],
      "metadata": {
        "id": "FFEtxL7FoQoM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train.relevent_experience.unique()"
      ],
      "metadata": {
        "trusted": true,
        "id": "DwRsGwyCoQoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We give a value of 0 if they have no experience and a value of 1 if they do"
      ],
      "metadata": {
        "id": "-6MCsmL0oQoM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Check value, if \"has relevent experience\" turn the value into 1, if not 0\n",
        "#Apply to both dataframes\n",
        "train[\"relevent_experience\"] = train.relevent_experience.apply(lambda x: 1 if str(x) == 'Has relevent experience' else 0)\n",
        "test[\"relevent_experience\"] = test.relevent_experience.apply(lambda x: 1 if str(x) == 'Has relevent experience' else 0)"
      ],
      "metadata": {
        "trusted": true,
        "id": "55fr_YQvoQoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check to make sure we only have 0 and 1\n",
        "train.relevent_experience.unique()"
      ],
      "metadata": {
        "trusted": true,
        "id": "NclLvbNVoQoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Target Encoding\n",
        "\n",
        "We'll take a look at the amount and rarity of the remaining columns."
      ],
      "metadata": {
        "id": "Vxpw-cm1oQoN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train.select_dtypes([\"object\"]).nunique()"
      ],
      "metadata": {
        "trusted": true,
        "id": "KL5djtxzoQoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that city has a large amount of different values. Let's take a look at the distribution between them."
      ],
      "metadata": {
        "id": "1k4lWirroQoN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train[\"city\"].value_counts()"
      ],
      "metadata": {
        "trusted": true,
        "id": "hAcRSLEmoQoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have quite a few rarely occuring values. Because of this this feature would be a good fit for target encoding."
      ],
      "metadata": {
        "id": "tpHcj0LGoQoN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_c = train.copy()\n",
        "target_train = train_c.pop(\"target\")\n",
        "\n",
        "enc_train = train_c.sample(frac=0.15)\n",
        "enc_target = target_train[enc_train.index]\n",
        "\n",
        "train_pre = train_c.drop(enc_train.index)\n",
        "target_pre = target_train[train_pre.index]"
      ],
      "metadata": {
        "trusted": true,
        "id": "SC9dQv1coQoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "enc = MEstimateEncoder(cols=[\"city\"],m=6)\n",
        "\n",
        "enc.fit(enc_train,enc_target)\n",
        "\n",
        "train_aff = enc.transform(train_pre)\n",
        "train_aff[\"target\"] = target_train\n",
        "train = train_aff"
      ],
      "metadata": {
        "trusted": true,
        "id": "FZf7yg84oQoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Updating lists\n",
        "\n",
        "We need to update the categorical and numerical columns lists."
      ],
      "metadata": {
        "id": "JYqwAetVoQoN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#List the columns we've encoded in the last steps\n",
        "encoded = [\"last_new_job\",\"experience\",\"relevent_experience\",\"company_size\",\"city\"]\n",
        "#Also list the new ones we've created\n",
        "new = [\"last_new_job_>4\",\"last_new_job_never\",\"experience_>20\",\"experience_<1\",\"company_size_encoded\"]\n",
        "for col in encoded:\n",
        "    cat_cols.remove(col)\n",
        "for col in new:\n",
        "    num_cols.append(col)\n",
        "for col in encoded:\n",
        "    num_cols.append(col)\n",
        "\n",
        "num_cols.remove(\"company_size\")\n",
        "cat_cols.remove(\"major_discipline\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "MQZtZvuOoQoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"cat_cols: {}\\nnum_cols: {}]\".format(cat_cols,num_cols))"
      ],
      "metadata": {
        "trusted": true,
        "id": "oIN-iuetoQoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Algo OH-Encoding\n",
        "\n",
        "Now we'll OneHot-encode the rest of the categorical columns.\n",
        "\n",
        "We start by creating the OH-Encoder and creating copys of our dataframes."
      ],
      "metadata": {
        "id": "fcGDZwrxoQoO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "oh = OneHotEncoder(sparse=False)\n",
        "#Create copys to work on\n",
        "train_c = train.copy()\n",
        "test_c = test.copy()\n",
        "#Fit and transform on training set\n",
        "train_c_cats = pd.DataFrame(oh.fit_transform(train_c[cat_cols]))\n",
        "#Transform test set\n",
        "test_c_cats = pd.DataFrame(oh.transform(test_c[cat_cols]))\n",
        "\n",
        "#Put column names back\n",
        "train_c_cats.columns = oh.get_feature_names(cat_cols)\n",
        "test_c_cats.columns = oh.get_feature_names(cat_cols)\n",
        "\n",
        "#Put the index back\n",
        "train_c_cats.index = train_c.index\n",
        "test_c_cats.index = test_c.index\n",
        "\n",
        "#Drop the old categorical values\n",
        "train_c.drop(cat_cols,axis=1,inplace=True)\n",
        "test_c.drop(cat_cols,axis=1,inplace=True)\n",
        "\n",
        "#Create the new dataframes\n",
        "train_oh = pd.concat([train_c,train_c_cats],axis=1)\n",
        "test_oh = pd.concat([test_c,test_c_cats],axis=1)"
      ],
      "metadata": {
        "trusted": true,
        "id": "Vgh1VjxfoQoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking data types\n",
        "\n",
        "We can check to make sure we only have numerical data by looking at data types"
      ],
      "metadata": {
        "id": "zIclfVJ1oQoO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_oh.dtypes"
      ],
      "metadata": {
        "trusted": true,
        "id": "tQcuuTCPoQoO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see here that the columns we manually encoded still have `object` datatype. We can fix this by converting all columns to floats."
      ],
      "metadata": {
        "id": "JUmC9O8JoQoO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dict to store our columns to conver\n",
        "dtypes = {}\n",
        "\n",
        "# Go through all the numerical columns\n",
        "for col in num_cols:\n",
        "    # Set the column to be converted to float\n",
        "    dtypes[col] = float\n",
        "\n",
        "# Convert training and testing data\n",
        "train_oh = train_oh.astype(dtypes)\n",
        "# We can only target encode once we have the targets at the end of the file\n",
        "dtypes.pop(\"city\")\n",
        "test_oh = test_oh.astype(dtypes)"
      ],
      "metadata": {
        "trusted": true,
        "id": "pYbjgw1IoQoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_oh.dtypes"
      ],
      "metadata": {
        "trusted": true,
        "id": "luOK8GaaoQoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_oh.dtypes"
      ],
      "metadata": {
        "trusted": true,
        "id": "1rgef8HFoQoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Examining the target and fixing imbalance\n",
        "\n",
        "We seperate the target and check how many true values we have compared to false"
      ],
      "metadata": {
        "id": "DjiJR3FjoQoP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Fetch target from training data\n",
        "y = train_oh.target\n",
        "#Drop the target from the original training data\n",
        "train_oh.drop(\"target\",inplace=True,axis=1)"
      ],
      "metadata": {
        "trusted": true,
        "id": "HJOnGqTAoQoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Now chech for imbalance\n",
        "sns.countplot(x=y)\n",
        "f = len([x for x in y if x == 0])\n",
        "t = len([x for x in y if x == 1])\n",
        "print(r\"False: {f}\" \"\\n\" \"True: {t}\" \"\\n\" \"Percent of true: {p:%}\".format(f = f, t=t,p= round(t/f,2)))"
      ],
      "metadata": {
        "trusted": true,
        "id": "RSuiAEAhoQoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset has enough of an unbalance that using smote will help us train the algo"
      ],
      "metadata": {
        "trusted": true,
        "id": "P8FmGxTmoQoQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sm = SMOTE(random_state=1)\n",
        "sm_train, sm_target = sm.fit_resample(train_oh,y)"
      ],
      "metadata": {
        "trusted": true,
        "id": "DrJuqkntoQoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model creation, prediction and assesment\n",
        "\n",
        "## Model creation\n",
        "\n",
        "We start by splitting the data, so that we can measure the models performance."
      ],
      "metadata": {
        "id": "ca0cHYnsoQoQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train,x_test,y_train,y_test = train_test_split(sm_train, sm_target,random_state=1,test_size=0.3)"
      ],
      "metadata": {
        "trusted": true,
        "id": "BFfaXX3ToQoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparameter tuning\n",
        "\n",
        "We'll use hyperparameter tuning to create and tune our model"
      ],
      "metadata": {
        "id": "ghg7o4rKoQoQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Setup the tuner\n",
        "def bayes_tuner(X, y, init_round=15, opt_round=25, n_folds=3, random_seed=6,n_estimators=10000, output_process=False):\n",
        "    #Prep our data\n",
        "    lgb_train = lgb.Dataset(data=x_train, label=y_train, free_raw_data=False)\n",
        "    #Setup the parameters\n",
        "    def lgb_eval(learning_rate,num_leaves, feature_fraction, max_depth, max_bin, min_data_in_leaf,min_sum_hessian_in_leaf,subsample):\n",
        "        params = {'application':'binary', 'metric':'auc'}\n",
        "        params['learning_rate'] = max(min(learning_rate, 1), 0)\n",
        "        params[\"num_leaves\"] = int(round(num_leaves))\n",
        "        params['feature_fraction'] = max(min(feature_fraction, 1), 0)\n",
        "        params['max_depth'] = int(round(max_depth))\n",
        "        params['max_bin'] = int(round(max_depth))\n",
        "        params['min_data_in_leaf'] = int(round(min_data_in_leaf))\n",
        "        params['min_sum_hessian_in_leaf'] = min_sum_hessian_in_leaf\n",
        "        params['subsample'] = max(min(subsample, 1), 0)\n",
        "\n",
        "        #Get the results with cv and return them\n",
        "        cv_result = lgb.cv(params, lgb_train, nfold=n_folds, seed=random_seed, stratified=True, verbose_eval =200, metrics=['auc'])\n",
        "        return max(cv_result['auc-mean'])\n",
        "\n",
        "    #Setup the optimizer\n",
        "    optimizer = BayesianOptimization(lgb_eval, {'learning_rate': (0.01, 1.0),\n",
        "                                            'num_leaves': (24, 80),\n",
        "                                            'feature_fraction': (0.1, 0.9),\n",
        "                                            'max_depth': (5, 30),\n",
        "                                            'max_bin':(20,90),\n",
        "                                            'min_data_in_leaf': (20, 200),\n",
        "                                            'min_sum_hessian_in_leaf':(0,100),\n",
        "                                           'subsample': (0.01, 1.0)}, random_state=0)\n",
        "\n",
        "    #Maximize score\n",
        "    optimizer.maximize(init_points=init_round, n_iter=opt_round)\n",
        "\n",
        "    #Fetch the auc scores\n",
        "    model_auc=[]\n",
        "    for model in range(len(optimizer.res)):\n",
        "        model_auc.append(optimizer.res[model]['target'])\n",
        "\n",
        "    #Fetch the best param\n",
        "    return optimizer.res[pd.Series(model_auc).idxmax()]['target'],optimizer.res[pd.Series(model_auc).idxmax()]['params']\n",
        "\n",
        "#Finally run the optimizer to get the best params for our model\n",
        "opt_params = bayes_tuner(x_train, y_train, init_round=5, opt_round=10, n_folds=4, random_seed=0,n_estimators=10000)\n",
        "opt_params = opt_params[1]"
      ],
      "metadata": {
        "_kg_hide-output": true,
        "scrolled": true,
        "trusted": true,
        "id": "Xy8JCgSeoQoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model setup and training\n",
        "\n",
        "Now we create the model. \\\n",
        "We'll be using LGBMClassifier"
      ],
      "metadata": {
        "id": "uVeBpBMyoQoQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "opt_params['num_leaves'] = int(round(opt_params['num_leaves'],2))\n",
        "opt_params['max_depth'] = int(round(opt_params['max_depth'],2))\n",
        "opt_params['min_data_in_leaf'] = int(round(opt_params['min_data_in_leaf'],2))\n",
        "opt_params['max_bin'] = int(round(opt_params['max_bin'],2))\n",
        "opt_params['metric'] = 'auc'\n",
        "opt_params['objective'] = \"binary\"\n",
        "opt_params"
      ],
      "metadata": {
        "trusted": true,
        "id": "pc4liDlRoQoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_data = lgb.Dataset(x_train,y_train)\n",
        "testing_data = lgb.Dataset(x_test,y_test)\n",
        "num_rounds = 15000\n",
        "clf = lgb.train(opt_params,training_data,num_rounds,valid_sets=[training_data,testing_data],verbose_eval = 500, early_stopping_rounds=250)"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "scrolled": true,
        "_kg_hide-output": true,
        "trusted": true,
        "id": "8Q2fo2xkoQoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Make predictions using the testing data\n",
        "y_pred = clf.predict(x_test,num_iteration=clf.best_iteration)\n",
        "#Also make predictions with the training data for comparison\n",
        "y_train_pred = clf.predict(x_train,num_iteration=clf.best_iteration)"
      ],
      "metadata": {
        "trusted": true,
        "id": "t0lPzcAooQoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assesment\n",
        "\n",
        "We'll asses the models performance using multiple different metrics, starting with roc_auc_score\n",
        "### roc_auc_score"
      ],
      "metadata": {
        "id": "DNPExnR6oQoR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Fetch acc score for test and training predictions\n",
        "acc_test = roc_auc_score(y_test, y_pred)\n",
        "acc_train = roc_auc_score(y_train,y_train_pred)\n",
        "\n",
        "print(r\"Test accuracy: {test}\" \"\\n\" \"Train accuracy: {train}\".format(test=acc_test,train=acc_train))"
      ],
      "metadata": {
        "trusted": true,
        "id": "LtQS5pLwoQoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Confusion matrix\n",
        "\n",
        "We'll also use a confusion matrix to see how many false positives and false negatives we got"
      ],
      "metadata": {
        "id": "xUIU6HW2oQoR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Make preds into 0 or 1\n",
        "y_pred_rounded = y_pred.round()\n",
        "#Create the confusion matrix, and normalize results to easily see percentages\n",
        "cm = confusion_matrix(y_test,y_pred_rounded,normalize=\"all\")\n",
        "#Create a easy to display version of the confusion matrix\n",
        "cm_disp = ConfusionMatrixDisplay(confusion_matrix = cm)\n",
        "#Display the matrix\n",
        "cm_disp.plot()"
      ],
      "metadata": {
        "trusted": true,
        "id": "sTcd9S29oQoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final submission\n",
        "\n",
        "Now we'll create the final submission file for the task.\\\n",
        "We then store the predictions in a dataframe with the IDs"
      ],
      "metadata": {
        "id": "iyGOcrWmoQoR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We read in the final targets\n",
        "final_targets = pd.DataFrame(np.load(\"../input/job-change-dataset-answer/jobchange_test_target_values.npy\"),columns=[\"target\"])\n",
        "\n",
        "# We need to take out targets for rows that were dropped\n",
        "for indx in final_targets.index:\n",
        "    # If the indx is in the preds then we do nothing\n",
        "    if indx in test_oh.index:\n",
        "        continue\n",
        "    else:\n",
        "        # If it isn't then drop it\n",
        "        final_targets.drop(index=indx,inplace=True)\n",
        "\n",
        "final_targets"
      ],
      "metadata": {
        "trusted": true,
        "id": "ZXciJO58oQoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a copy of the dataframe to work on\n",
        "test_c = test_oh.copy()\n",
        "# We also make a copy of the targets\n",
        "targets = final_targets.copy()\n",
        "\n",
        "# Take the targets out to index by them\n",
        "target_test = targets.pop(\"target\")\n",
        "# Take out a sample of the dataframe for encoding\n",
        "enc_test = test_c.sample(frac=0.3)\n",
        "\n",
        "# Take a the targets that match the testing data index\n",
        "enc_target = target_test[enc_test.index]\n",
        "\n",
        "# Take out the data used for encoding\n",
        "test_pre = test_c.drop(enc_test.index)\n",
        "# Take out the targets used for encoding\n",
        "target_pre = target_test[test_pre.index]"
      ],
      "metadata": {
        "trusted": true,
        "id": "hjmGklPBoQoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup the encoder\n",
        "enc = MEstimateEncoder(cols=[\"city\"],m=6)\n",
        "\n",
        "# Fit the encoder with the data we split for encoding\n",
        "enc.fit(enc_test,enc_target)\n",
        "\n",
        "# Use the encoder to transform the pretest data\n",
        "test_aff = enc.transform(test_pre)\n",
        "# Set the the test data to the transformed one\n",
        "test = test_aff"
      ],
      "metadata": {
        "trusted": true,
        "id": "tIjDiHbLoQoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a predictions on the test data\n",
        "preds = clf.predict(test)\n",
        "\n",
        "#Turn it into a dataframe, with the corresponding id\n",
        "submission_preds= pd.DataFrame({'enrollee_id': test.enrollee_id,'target':preds})\n",
        "submission_preds"
      ],
      "metadata": {
        "trusted": true,
        "id": "WUaaP7_IoQoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the predictions for submission\n",
        "submission_preds.to_csv('submission_preds.csv',index=False)"
      ],
      "metadata": {
        "trusted": true,
        "id": "dxk7adbDoQoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We need to take out targets for rows that were used in encoding\n",
        "for indx in final_targets.index:\n",
        "    # If the indx is in the preds then we do nothing\n",
        "    if indx in test.index:\n",
        "        continue\n",
        "    else:\n",
        "        # If it isn't then drop it\n",
        "        final_targets.drop(index=indx,inplace=True)\n",
        "\n",
        "final_targets"
      ],
      "metadata": {
        "trusted": true,
        "id": "g_oABLKIoQoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Fetch acc score\n",
        "acc = roc_auc_score(final_targets, preds)\n",
        "\n",
        "print(r\"Test accuracy: {}\".format(acc))"
      ],
      "metadata": {
        "trusted": true,
        "id": "fxgVLS3BoQoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Examining our model\n",
        "\n",
        "Now that we've set the model up we can take a look at some different metrics to see what parts of the data actually drive the model. \\\n",
        "In a real life scenario this would be the most valuable part.\n",
        "\n",
        "## SHAP\n",
        "\n",
        "We'll start by looking at shap values"
      ],
      "metadata": {
        "id": "vJuX6BNJoQoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up a variable to easily look at different samples of data\n",
        "x = 2\n",
        "# Setup an explainer\n",
        "exp = shap.TreeExplainer(clf)\n",
        "\n",
        "# Fetch a part of the data to look at\n",
        "plot_data = test[x:x+1]\n",
        "\n",
        "# Get the shap values for the selected data\n",
        "shaps = exp.shap_values(plot_data)\n",
        "# Init JavaScript so we can disp. the plot\n",
        "shap.initjs()\n",
        "\n",
        "# Create a plot to see how each feature affects the prediction\n",
        "shap.force_plot(exp.expected_value[1], shaps[1], plot_data)"
      ],
      "metadata": {
        "trusted": true,
        "id": "ZhZ3dAmYoQoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Permutation importance\n",
        "\n",
        "We can also check permutation impo"
      ],
      "metadata": {
        "id": "lq6sX4ScoQoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the permutation importance scores for each feature\n",
        "lgb.plot_importance(clf)"
      ],
      "metadata": {
        "trusted": true,
        "id": "ObsARkHFoQoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decision tree\n",
        "We can also plot our model into a decision tree."
      ],
      "metadata": {
        "id": "ukZyLFW_oQoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup a tree graph for the model\n",
        "graph = lgb.create_tree_digraph(clf)\n",
        "# Set the graph size to slightly bigger\n",
        "graph.graph_attr.update(size=\"110,110\")\n",
        "# Display the graph\n",
        "graph"
      ],
      "metadata": {
        "trusted": true,
        "id": "ZsUZHETWoQoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From all of these, we can figure that the experience of the applicant is quite important. Based on shap values the more experience an applicant has the lower the chance that they're loking for a job change. It also seems that the training hours spent on an applicant heavily increase the chance that they're looking for a job change."
      ],
      "metadata": {
        "id": "sw_yMgpyoQoU"
      }
    }
  ]
}